{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import jieba\n",
    "from random import shuffle\n",
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一对 乌鸦 飞 到 我们 屋顶 上 的 巢里 ， 它们 好像 专门 为拉木 而 来 的 。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(jieba.cut('一对乌鸦飞到我们屋顶上的巢里，它们好像专门为拉木而来的。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_path = '/home/code-master/Documents/deeplearningProject/translate/ai_challenger_translation_train_20170904/translation_train_data_20170904/train.zh'\n",
    "en_path = '/home/code-master/Documents/deeplearningProject/translate/ai_challenger_translation_train_20170904/translation_train_data_20170904/train.en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = re.sub(r'([?!,.，。！？])', r' \\1 ', w) # 在单词与其后的标点符号间插入空格\n",
    "    w = re.sub(r'[\" \"]+', ' ', w)\n",
    "    w = re.sub(r'[^a-zA-Z\\u4e00-\\u9fa5?!,.，。！？]+', ' ',w) # 将所有不相关的字符替换为空格\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>' # 给句子前后加上开始和结束预测\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.582 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "zh_sentence = ' '.join(jieba.cut('一对乌鸦飞到我们屋顶上的巢里，它们好像专门为拉木而来的。'))\n",
    "en_sentence = 'A pair of crows had come to nest on our roof as if they had come for Lhamo.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> 一对 乌鸦 飞 到 我们 屋顶 上 的 巢里 ， 它们 好像 专门 为拉木 而 来 的 。 <end>\n",
      "<start> A pair of crows had come to nest on our roof as if they had come for Lhamo . <end>\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(zh_sentence))\n",
    "print(preprocess_sentence(en_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(num_example):\n",
    "    zh_lines = open(zh_path).read().split('\\n')\n",
    "    en_lines = open(en_path).read().split('\\n')\n",
    "    inds = np.random.permutation(len(zh_lines))\n",
    "    zh_lang = [preprocess_sentence(' '.join(jieba.cut(zh_lines[i]))) for i in inds[:num_example]]\n",
    "    en_lang = [preprocess_sentence(en_lines[i]) for i in inds[:num_example]]\n",
    "    return zh_lang, en_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh, en = create_dataset(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> 你 不仅仅 想要 做 化疗 。 <end>\n",
      "<start> You don t just want chemo . <end>\n"
     ]
    }
   ],
   "source": [
    "print(zh[-1])\n",
    "print(en[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(data, path):\n",
    "    with open(path, mode='w') as wf:\n",
    "        wf.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data('\\n'.join(zh), './data/mini_zh_en/train.zh')\n",
    "write_data('\\n'.join(en), './data/mini_zh_en/train.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> 你 不仅仅 想要 做 化疗 。 <end>\n",
      "<start> You don t just want chemo . <end>\n"
     ]
    }
   ],
   "source": [
    "zh_lang = open('data/mini_zh_en/train.zh').read().split('\\n')\n",
    "en_lang = open('data/mini_zh_en/train.en').read().split('\\n')\n",
    "print(zh_lang[-1])\n",
    "print(en_lang[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenize = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenize.fit_on_texts(lang)\n",
    "    tensor = lang_tokenize.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(num_examples):\n",
    "    #zh_lang, en_lang = create_dataset(num_examples)\n",
    "    zh_lang = open('data/mini_zh_en/train.zh').read().split('\\n')\n",
    "    en_lang = open('data/mini_zh_en/train.en').read().split('\\n')\n",
    "    zh_tensor, zh_tokenize = tokenize(zh_lang)\n",
    "    en_tensor, en_tokenize = tokenize(en_lang)\n",
    "    return zh_tensor, en_tensor, zh_tokenize, en_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 30000\n",
    "zh_tensor, en_tensor, zh_tokenize, en_tokenize = load_dataset(num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,   219,  3149,  2364,  3515,  9154,  1795,  3150,     4,\n",
       "        14157,  6749,     5,    10,  3957,   396, 14158,     4,  1504,\n",
       "          216,     4,   743, 14159,     3,     2,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [    1,     6,   229,    60,    10,   175,    21,   302,     2,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [    1,    14,    19,  1321,  2556,     3,    33,    75,  9155,\n",
       "            4,  9156,     3,     2,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_tensor[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    2,    23,  3290,    23,    18,  1533,  4205,     4,   300,\n",
       "            4,     5,  5861,   551,  2301,  1473,     4,     9,  1080,\n",
       "        10910,  4864,     5,   335,    26,   300,    10,     5,   166,\n",
       "            1,     3,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [    2,     6,   176,    14,    25,     9,    92,   255,     1,\n",
       "            3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [    2,     6,  1474,    20,    24,     9,   531,   111,     1,\n",
       "           14,    13,  7456,    13, 10911,     1,     3,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tensor[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(token, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(t,token.index_word[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <start>\n",
      "6 我\n",
      "229 以为\n",
      "60 那\n",
      "10 是\n",
      "175 所\n",
      "21 好\n",
      "302 学校\n",
      "2 <end>\n",
      "2 <start>\n",
      "6 i\n",
      "176 thought\n",
      "14 it\n",
      "25 was\n",
      "9 a\n",
      "92 good\n",
      "255 school\n",
      "1 .\n",
      "3 <end>\n"
     ]
    }
   ],
   "source": [
    "convert(zh_tokenize, zh_tensor[1])\n",
    "convert(en_tokenize, en_tensor[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 66) (30000, 118)\n"
     ]
    }
   ],
   "source": [
    "print(zh_tensor.shape, en_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_tensor_train, zh_tensor_val, en_tensor_train, en_tensor_val = train_test_split(zh_tensor, en_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 66) (6000, 66) (24000, 118) (6000, 118)\n"
     ]
    }
   ],
   "source": [
    "print(zh_tensor_train.shape, zh_tensor_val.shape, en_tensor_train.shape, en_tensor_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build train set by tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(zh_tensor_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((zh_tensor_train, en_tensor_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_zh_batch, example_en_batch = next(iter(dataset.take(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 66) (64, 118)\n"
     ]
    }
   ],
   "source": [
    "print(example_zh_batch.shape, example_en_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1  212 6334 ...    0    0    0]\n",
      " [   1    7   77 ...    0    0    0]\n",
      " [   1    6  463 ...    0    0    0]\n",
      " ...\n",
      " [   1  120   49 ...    0    0    0]\n",
      " [   1   16 1295 ...    0    0    0]\n",
      " [   1   58   10 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(example_zh_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_train_lens = zh_tensor_train.shape[1]\n",
    "en_train_lens = en_tensor_train.shape[1]\n",
    "vocab_zh_size = len(zh_tokenize.word_index) + 1\n",
    "vocab_en_size = len(en_tokenize.word_index) + 1\n",
    "emb_size = 256\n",
    "units = 1024\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 118 34672 23595\n"
     ]
    }
   ],
   "source": [
    "print(zh_train_lens, en_train_lens, vocab_zh_size, vocab_en_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "inp_enc = tf.keras.layers.Input(shape=(zh_train_lens,))\n",
    "enc_emb = tf.keras.layers.Embedding(vocab_zh_size, emb_size)(inp_enc)\n",
    "inp_out, h_state, c_state = tf.keras.layers.LSTM(units, return_state=True)(enc_emb)\n",
    "\n",
    "# decoder\n",
    "inp_dec = tf.keras.layers.Input(shape=(en_train_lens,))\n",
    "dec_emb = tf.keras.layers.Embedding(vocab_en_size, emb_size)(inp_dec)\n",
    "dec_lstm = tf.keras.layers.LSTM(units, return_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_lstm.states[0] = inp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_out = dec_lstm(dec_emb)\n",
    "out = tf.keras.layers.Dense(vocab_en_size)(dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model([inp_enc, inp_dec], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 118)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 118, 256)     6040320     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 118, 1024)    5246976     embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 66)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 118, 23595)   24184875    lstm_7[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 35,472,171\n",
      "Trainable params: 35,472,171\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
